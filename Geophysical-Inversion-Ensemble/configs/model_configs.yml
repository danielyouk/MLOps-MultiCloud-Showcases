# configs/model_configs.yml
# This file defines the portfolio of models for the training experiments.
# Each top-level key represents a unique experiment or model variant that will be
# submitted as a distinct job to Azure ML.
#
# The orchestrator script (`scripts/train_all_models.py`) reads this file
# and launches one job for each model defined below.
#
# --- Key Parameters ---
# model_name: The specific model architecture from the timm library.
# learning_rate: The initial learning rate for the optimizer.
# batch_size: The number of samples per batch. Adjust based on GPU memory.
# num_epochs: The maximum number of epochs for training.
# early_stopping_patience: Number of epochs with no improvement to wait before stopping.
# early_stopping_min_delta: Minimum change in validation loss to be considered an improvement.
# compute_target: (Optional) Specify a specific Azure ML compute cluster for this job.
#                 If omitted, the default cluster from the orchestrator script is used.
#                 This is useful for assigning larger models to more powerful GPUs.

# --- Group 1: CAFormer Family ---
caformer_b36:
  model_name: caformer_b36.sail_in22k_ft_in1k
  learning_rate: 0.0001
  batch_size: 8
  num_epochs: 100
  early_stopping_patience: 10
  early_stopping_min_delta: 0.0001

caformer_s36:
  model_name: caformer_s36.sail_in22k_ft_in1k
  learning_rate: 0.0001
  batch_size: 8
  num_epochs: 100
  early_stopping_patience: 10
  early_stopping_min_delta: 0.0001

caformer_s18:
  model_name: caformer_s18.sail_in22k_ft_in1k
  learning_rate: 0.0001
  batch_size: 8
  num_epochs: 100
  early_stopping_patience: 10
  early_stopping_min_delta: 0.0001

# --- Group 2: ConvNeXt Family ---
convnext_base:
  model_name: convnext_base.fb_in22k_ft_in1k
  learning_rate: 0.0001
  batch_size: 8
  num_epochs: 100
  early_stopping_patience: 10
  early_stopping_min_delta: 0.0001

convnext_large:
  model_name: convnext_large.fb_in22k_ft_in1k
  learning_rate: 0.0001
  batch_size: 4
  compute_target: my-2-node-powerful-cluster
  num_epochs: 100
  early_stopping_patience: 10
  early_stopping_min_delta: 0.0001

convnext_tiny:
  model_name: convnext_tiny.fb_in22k_ft_in1k
  learning_rate: 0.0001
  batch_size: 32
  num_epochs: 100
  early_stopping_patience: 10
  early_stopping_min_delta: 0.0001

convnextv2_base:
  model_name: convnextv2_base.fcmae_ft_in22k_in1k
  learning_rate: 0.0001
  batch_size: 8
  num_epochs: 100
  early_stopping_patience: 10
  early_stopping_min_delta: 0.0001

convnextv2_atto:
  model_name: convnextv2_atto.fcmae_ft_in1k
  learning_rate: 0.0001
  batch_size: 8
  num_epochs: 100
  early_stopping_patience: 10
  early_stopping_min_delta: 0.0001
  
# --- Group 3: Hybrid Architectures ---
maxvit_base:
  model_name: maxvit_base_tf_512.in21k_ft_in1k
  learning_rate: 0.0001
  batch_size: 8
  num_epochs: 100
  early_stopping_patience: 10
  early_stopping_min_delta: 0.0001

coatnet_2:
  model_name: coatnet_2_rw_224
  learning_rate: 0.0001
  batch_size: 8
  num_epochs: 100
  early_stopping_patience: 10
  early_stopping_min_delta: 0.0001

coatnext_nano:
  model_name: coatnext_nano_rw_224.sw_in1k
  learning_rate: 0.0001
  batch_size: 8
  num_epochs: 100
  early_stopping_patience: 10
  early_stopping_min_delta: 0.0001

# --- Group 4: Efficient & Vision Transformers ---
efficientnetv2_l:
  model_name: tf_efficientnetv2_l.in21k_ft_in1k
  learning_rate: 0.0001
  batch_size: 16
  compute_target: my-2-node-powerful-cluster
  num_epochs: 100
  early_stopping_patience: 10
  early_stopping_min_delta: 0.0001

mobilenetv3_large:
  model_name: mobilenetv3_large_100.miil_in21k_ft_in1k
  learning_rate: 0.0001
  batch_size: 32
  compute_target: my-2-node-powerful-cluster
  num_epochs: 100
  early_stopping_patience: 10
  early_stopping_min_delta: 0.0001

vit_base:
  model_name: vit_base_patch16_224.augreg_in21k_ft_in1k
  learning_rate: 0.0001
  batch_size: 8
  num_epochs: 100
  early_stopping_patience: 10
  early_stopping_min_delta: 0.0001

vit_small:
  model_name: vit_small_patch16_224.augreg_in21k_ft_in1k
  learning_rate: 0.0001
  batch_size: 8
  num_epochs: 100
  early_stopping_patience: 10
  early_stopping_min_delta: 0.0001
  
# --- Group 5: Wildcard ---
eva02_base:
  model_name: eva02_base_patch14_224.mim_in22k
  learning_rate: 0.0001
  batch_size: 8
  num_epochs: 100
  early_stopping_patience: 10
  early_stopping_min_delta: 0.0001